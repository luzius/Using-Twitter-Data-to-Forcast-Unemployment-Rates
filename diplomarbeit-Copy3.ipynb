{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Use Twitter Data to Forcast Unemployment Rates\n",
    "#-----------------------------------------------------------\n",
    "__author__ = 'Luzius von Gunten'\n",
    "#-----\n",
    "#Description:\n",
    "\n",
    "#-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#non-standard packages originating from other develogppers:\n",
    "#---\n",
    "#:::TwitterScraper:::\n",
    "#by Tom Dickinson, some changes by Luzius von Gunten\n",
    "#https://github.com/tomkdickinson/Twitter-Search-API-Python/blob/master/TwitterScraper.py\n",
    "import searchTwi as st \n",
    "#---\n",
    "#standard packages:\n",
    "#---\n",
    "from calendar import monthrange\n",
    "from datetime import date\n",
    "from monthdelta import monthdelta\n",
    "from monthdelta import monthmod\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1. Functions to scrape twitter data by twitter advanced search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def QueryListBuilder(keywords, location_ref):\n",
    "    '''\n",
    "    build the queries for extracting twitter messages, crossproduct of the two parameters is stored in a list\n",
    "    parmas:\n",
    "    - keywords: list of search terms\n",
    "    - location_ref: list of terms which refer to locations, such as 'schweiz'\n",
    "    '''\n",
    "    return [k + l for l in location_ref for k in keywords]\n",
    "\n",
    "def RunQueries(querylist,max_tweet,from_y,from_m,to_y,to_m):\n",
    "    '''\n",
    "    - executes twitter message search on basis twitter expert search:\n",
    "        https://github.com/tomkdickinson/Twitter-Search-API-Python/blob/master/TwitterScraper.py\n",
    "    - within each month between start and end month, each query in the list of queries is executed separatly, with \n",
    "      a stated maximum of tweets to be extracted per query and month\n",
    "    - parmas:\n",
    "        - querylist: list of search terms --> use 'QueryListBuilder'\n",
    "        - max_tweet: maximum of tweets to be extracted per query and month\n",
    "        - from_y/from_m: start year/month\n",
    "        - to_y/to_m: end year/month\n",
    "    '''    \n",
    "    startMonth = date(from_y,from_m,1)\n",
    "    currentMonth = startMonth\n",
    "    endMonth = date(to_y,to_m,1)\n",
    "    i = 0\n",
    "    tweets = []\n",
    "    nTweetsPerMonth = []\n",
    "    while currentMonth <> endMonth:\n",
    "        currentMonth = startMonth + monthdelta(i)\n",
    "        firstDayInMonth = str(currentMonth)\n",
    "        lastDayInMonth = str(date(currentMonth.year,currentMonth.month,monthrange(currentMonth.year,currentMonth.month)[1]))\n",
    "        print \"--- %s ---\" % str(currentMonth)\n",
    "        monthlyTweets = []\n",
    "        for q in querylist:\n",
    "            query = q + ' since:' + firstDayInMonth + ' until:' + lastDayInMonth\n",
    "            search1 = st.TwitterSearchImpl(0, 5, max_tweet)\n",
    "            tweetsThisRound = search1.search(query)\n",
    "            \n",
    "            '''\n",
    "            dat = datetime.datetime.fromtimestamp((t['created_at']/1000))\n",
    "            fmt = \"%Y-%m-%d\"\n",
    "            print \"%s --- %s\" %(dat.strftime(fmt), t['text'])\n",
    "            '''\n",
    "            monthlyTweets.extend(tweetsThisRound)\n",
    "        nTweetsPerMonth.append(len(monthlyTweets))\n",
    "        print'number of tweets this month: %d' % len(monthlyTweets)\n",
    "        tweets.extend(monthlyTweets)\n",
    "        i = i + 1\n",
    "    print \"\\n--- \\ntotal number of tweets retrieved: %d\" % len(tweets)\n",
    "    print \"mean number of tweets per month: %d\" %np.mean(nTweetsPerMonth)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2. Functions to do basic data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(data, textField = 'text', deepCleaning = True):\n",
    "    '''\n",
    "    Removes new Line characters (\\n), Twitter usernames, URLs, and special characters via regex\n",
    "    Expects a list of dictionaries as an input, same data structure as output\n",
    "    params:\n",
    "    - data: list of dictionaries, each listelement is a dictionary with relevant fields (key) of information (value) per tweet\n",
    "    - textField: nominates the key where the corresponding value contains the tweet text\n",
    "    - clean: boolean, wether tweet text should be cleaned before sentimentanalysis (removes usernames, weblinks, new line tags and special characters)\n",
    "    '''\n",
    "    newLine = re.compile(r\"\\n|\\r\")\n",
    "    urls = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+=]|[!*\\(\\),\\?]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    usernames = re.compile(\"@[\\w_]*\")\n",
    "    specialChars1 = re.compile(r\"[/_\\-\\\\]\")\n",
    "    specialChars2 = re.compile(r\"[!<>«»“”#@:;.,.%()/'…\\[\\]\\?\\\"\\*]\")\n",
    "    for t in data:\n",
    "        t['cleanedText'] = newLine.sub(' ',t[textField]) \n",
    "        if  deepCleaning == True: \n",
    "            t['cleanedText'] = urls.sub(' ',t['cleanedText']) \n",
    "            t['cleanedText'] = usernames.sub(' ',t['cleanedText']) \n",
    "            t['cleanedText'] = specialChars1.sub(' ',t['cleanedText']) \n",
    "            t['cleanedText'] = specialChars2.sub('',t['cleanedText']).lower()\n",
    "    return data\n",
    "\n",
    "def keepFieldsInListOfDicts(keepFields,ListOfDicts):\n",
    "    data = [{n: t[n] for n in keepFields} for t in ListOfDicts]\n",
    "    return data\n",
    "\n",
    "def exporter(outfile,tweets, keepFields):\n",
    "    '''\n",
    "    stores extracted tweets as a json dump and for human readability in a csv-file\n",
    "    params:\n",
    "    - outfile = file to be writtein\n",
    "    - tweets = list of dictionaries with tweets\n",
    "    '''\n",
    "    #only keep wanted data fields\n",
    "    data = keepFieldsInListOfDicts(keepFields = keepFields, ListOfDicts = tweets)\n",
    "    \n",
    "    #save as json-file\n",
    "    with open('outdata/'+outfile+'.json', 'w') as fileout:\n",
    "        json.dump(data, fileout)\n",
    "    \n",
    "    #save as csv via pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('outdata/'+outfile+\".csv\", sep=';',quoting=csv.QUOTE_NONNUMERIC, encoding='utf8')\n",
    "    \n",
    "    print '\\n--- \\nDone. %d saved to disk.' %len(tweets)\n",
    "\n",
    "def getFromJSONDumpFile(jfile):\n",
    "    with open(jfile) as infile: \n",
    "        dat = json.load(infile)\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.3. Functions to do lexicon based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readSentiWS():\n",
    "    '''\n",
    "    reads SentiWS files (german dictionary for sentiment analysis) and outputs a dictionary \n",
    "    with words enlisted in SentiWS (key) and sentiment values (value)\n",
    "    http://asv.informatik.uni-leipzig.de/download/sentiws.html \n",
    "    '''\n",
    "    Neg = 'sentiment dict/SentiWS_v1.8c_Negative.txt' \n",
    "    Pos = 'sentiment dict/SentiWS_v1.8c_Positive.txt' \n",
    "    sentis = {}\n",
    "    for file in [Neg, Pos]:\n",
    "        f = codecs.open(file, encoding='utf-8')\n",
    "        newLine = re.compile(r'\\n')\n",
    "        pos = re.compile(r'\\|\\w+')\n",
    "        for line in f:\n",
    "            line = newLine.sub('',line) \n",
    "            line = pos.sub('',line) \n",
    "            split = re.split(',|\\t',line)\n",
    "            val = float(split[1])\n",
    "            del split[1]\n",
    "            for key in split:\n",
    "                sentis[key.lower()]=val\n",
    "        f.close()\n",
    "    return sentis\n",
    "\n",
    "def readGerPolClues():\n",
    "    '''\n",
    "    reads German Polarity Clues files (german dictionary for sentiment analysis) and outputs a dictionary \n",
    "    with words enlisted in German Polarity Clues (key) and sentiment values (value)\n",
    "    http://www.ulliwaltinger.de/sentiment/\n",
    "    '''\n",
    "    Neg = 'sentiment dict/GermanPolarityClues-Negative-21042012.tsv' \n",
    "    Pos = 'sentiment dict/GermanPolarityClues-Positive-21042012.tsv'\n",
    "    Neut = 'sentiment dict/GermanPolarityClues-Neutral-21042012.tsv'\n",
    "    sentis = {}\n",
    "    for file in [Neg, Pos, Neut]:\n",
    "        f = codecs.open(file, encoding='utf-8')\n",
    "        newLine = re.compile(r'\\n')\n",
    "        for line in f:\n",
    "            line = newLine.sub('',line) \n",
    "            split = re.split('\\t',line)\n",
    "            sentis[split[0].lower()]=split[3]\n",
    "        f.close()\n",
    "    return sentis\n",
    "\n",
    "def getSenti_sentiWS(lex, keys, tweet):\n",
    "    '''\n",
    "    get sentiment features for tweets based on SentiWS lexicon\n",
    "    params:\n",
    "    - lex: SentiWS lex as dictionary (key=words in lex, value = polarity of word), dict, output from \"readSentiWS()\"\n",
    "    - keys: words in SentiWS lex (lex.keys()), str\n",
    "    - tweet: tweet text, str\n",
    "    '''\n",
    "    tokens = filter(None, [x for x in tweet.split(' ')]) #tokenize tweets, filter empty strings (blanks)\n",
    "    matches = list(set(keys) & set(tokens)) #match tweet tokens and words in SentiWS-Lex\n",
    "    tweetFeatures = {}\n",
    "    if matches:\n",
    "        polarities = [lex[key] for key in matches] #get polarities from SentiWS for matched tweet tokens\n",
    "        tweetFeatures[\"sentiWS_nbrNeg\"] = sum(1 for n in polarities if n < 0)\n",
    "        tweetFeatures[\"sentiWS_nbrPos\"] = sum(1 for n in polarities if n > 0)\n",
    "        tweetFeatures[\"sentiWS_mean\"] = np.mean(polarities)\n",
    "        tweetFeatures[\"sentiWS_polarities\"] = polarities\n",
    "        if tweetFeatures[\"sentiWS_mean\"] < 0:\n",
    "            tweetFeatures[\"sentiWS_sentiVal\"] = 'negative'\n",
    "        elif tweetFeatures[\"sentiWS_mean\"] > 0:\n",
    "            tweetFeatures[\"sentiWS_sentiVal\"] = 'positive'\n",
    "        else:\n",
    "            tweetFeatures[\"sentiWS_sentiVal\"] = 'neutral'\n",
    "    else:\n",
    "        tweetFeatures = {\"sentiWS_nbrNeg\":0, \"sentiWS_nbrPos\":0, \"sentiWS_mean\":0, \"sentiWS_polarities\":[],\"sentiWS_sentiVal\":'neutral'}\n",
    "\n",
    "    return tweetFeatures\n",
    "\n",
    "def getSenti_GPC(lex, keys, tweet):\n",
    "    '''\n",
    "    get sentiment features for tweets based on German Polarity Clues (GPC) lexicon\n",
    "    params:\n",
    "    - lex: GPC lex as dictionary (key=words in lex, value = polarity of word), dict, output from \"readGerPolClues()\"\n",
    "    - keys: words in GPC lex (lex.keys()), str\n",
    "    - tweet: tweet text, str\n",
    "    '''\n",
    "    tokens = filter(None, [x for x in tweet.split(' ')]) #tokenize tweets, filter empty strings (blanks)\n",
    "    matches = list(set(keys) & set(tokens)) #match tweet tokens and words in SentiWS-Lex\n",
    "    tweetFeatures = {}\n",
    "    if matches:\n",
    "        polarities = [lex[key] for key in matches] #get polarities from SentiWS for matched tweet tokens\n",
    "        tweetFeatures[\"GPC_nbrNeg\"] = sum(1 for n in polarities if n == 'negative')\n",
    "        tweetFeatures[\"GPC_nbrPos\"] = sum(1 for n in polarities if n == 'positive')\n",
    "        tweetFeatures[\"GPC_nbrNeut\"] = sum(1 for n in polarities if n == 'neutral')\n",
    "        tweetFeatures[\"GPC_polarities\"] = polarities\n",
    "        if tweetFeatures[\"GPC_nbrNeg\"] > tweetFeatures[\"GPC_nbrPos\"]:\n",
    "            tweetFeatures[\"GPC_sentiVal\"] = 'negative'\n",
    "        elif tweetFeatures[\"GPC_nbrNeg\"] < tweetFeatures[\"GPC_nbrPos\"]:\n",
    "            tweetFeatures[\"GPC_sentiVal\"] = 'positive'\n",
    "        else: \n",
    "            tweetFeatures[\"GPC_sentiVal\"] = 'neutral'\n",
    "    else:\n",
    "        tweetFeatures = {\"GPC_nbrNeg\":0, \"GPC_nbrPos\":0, \"GPC_nbrNeut\":0, \"GPC_polarities\":[],\"GPC_sentiVal\":'neutral'}\n",
    "\n",
    "    return tweetFeatures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test run\n",
    "#RawTweets = RunQueries(querylist = ['arbeitsmarkt schweiz'], max_tweet=10, from_y=2012, from_m=1, to_y=2012, to_m=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.4. Function calls to collect and process raw tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### build queries\\nkeywords = [\\'arbeitslosigkeit\\',\\'wachstum\\',\\'arbeitsmarkt\\',\\'standort\\',\\'stellenmarkt\\',\\'werkplatz\\',\\'stellenabbau\\',\\n            \\'stellenausbau\\', \\'firmenschliessung\\', \\'standortverlagerung\\',\\'arbeitsplatzabbau\\', \\'personalabbau\\',\\n            \\'rationalisierungsmassnahme\\', \\'restrukturierung\\',\\'erwerbslosigkeit\\', \\'betriebsschliessung\\',\\n            \\'besch\\xc3\\xa4ftigung\\', \\'besch\\xc3\\xa4ftigte\\',\\'besch\\xc3\\xa4ftigten\\',\\n            \\'arbeitspl\\xc3\\xa4tze\\',\\'arbeitspl\\xc3\\xa4tze\\',\\n            \\'arbeitslose\\',\\'arbeitslosen\\',\\n            \\'stellensuchende\\',\\'stellensuchenden\\',\\n            \\'angestellten\\',\\'angestellte\\',\\n            \\'baut stellen ab\\', \\'schafft neue stellen\\']\\nlocation_ref = [\\' schweiz -#job -#jobs\\']\\n\\n#keywords=[\\'arbeit\\']\\n#location_ref = [\\' schweiz\\']\\n\\nQueries = QueryListBuilder(keywords = keywords, location_ref = location_ref )\\n\\n### get & clean tweets\\nRawTweets = RunQueries(querylist = Queries, max_tweet=200, from_y=2012, from_m=1, to_y=2016, to_m=10)\\nkeep = [\\'text\\',\\'created_at\\',\\'tweet_id\\',\\'retweets\\',\\'favorites\\',\\'user_name\\']\\nexporter(outfile = \"outputRaw_v4\", tweets = RawTweets, keepFields = keep)\\n\\nRawTweets = getFromJSONDumpFile(\\'outdata/outputRaw_v4.json\\') #reload saved tweets in order to keep only wanted fields\\nTweets = cleaner(RawTweets)\\n\\n### get sentiment dictionaries & determine tweet sentiment\\nsentiWS = readSentiWS()\\nsentiWSkeys = sentiWS.keys()\\ngpc = readGerPolClues()\\ngpcKeys = gpc.keys()\\nfor t in Tweets:\\n    feat1 = getSenti_sentiWS(lex = sentiWS, keys = sentiWSkeys, tweet = t[\\'cleanedText\\'])\\n    feat2 = getSenti_GPC(lex = gpc, keys = gpcKeys, tweet = t[\\'cleanedText\\'])\\n    t.update(feat1); t.update( feat2)\\n\\n### export tweets to file\\nkeep = Tweets[0].keys()\\nexporter(outfile = \"Tweets_v4\", tweets = RawTweets, keepFields = keep)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "### build queries\n",
    "keywords = ['arbeitslosigkeit','wachstum','arbeitsmarkt','standort','stellenmarkt','werkplatz','stellenabbau',\n",
    "            'stellenausbau', 'firmenschliessung', 'standortverlagerung','arbeitsplatzabbau', 'personalabbau',\n",
    "            'rationalisierungsmassnahme', 'restrukturierung','erwerbslosigkeit', 'betriebsschliessung',\n",
    "            'beschäftigung', 'beschäftigte','beschäftigten',\n",
    "            'arbeitsplätze','arbeitsplätze',\n",
    "            'arbeitslose','arbeitslosen',\n",
    "            'stellensuchende','stellensuchenden',\n",
    "            'angestellten','angestellte',\n",
    "            'baut stellen ab', 'schafft neue stellen']\n",
    "location_ref = [' schweiz -#job -#jobs']\n",
    "\n",
    "#keywords=['arbeit']\n",
    "#location_ref = [' schweiz']\n",
    "\n",
    "Queries = QueryListBuilder(keywords = keywords, location_ref = location_ref )\n",
    "\n",
    "### get & clean tweets\n",
    "RawTweets = RunQueries(querylist = Queries, max_tweet=200, from_y=2012, from_m=1, to_y=2016, to_m=10)\n",
    "keep = ['text','created_at','tweet_id','retweets','favorites','user_name']\n",
    "exporter(outfile = \"outputRaw_v4\", tweets = RawTweets, keepFields = keep)\n",
    "\n",
    "RawTweets = getFromJSONDumpFile('outdata/outputRaw_v4.json') #reload saved tweets in order to keep only wanted fields\n",
    "Tweets = cleaner(RawTweets)\n",
    "\n",
    "### get sentiment dictionaries & determine tweet sentiment\n",
    "sentiWS = readSentiWS()\n",
    "sentiWSkeys = sentiWS.keys()\n",
    "gpc = readGerPolClues()\n",
    "gpcKeys = gpc.keys()\n",
    "for t in Tweets:\n",
    "    feat1 = getSenti_sentiWS(lex = sentiWS, keys = sentiWSkeys, tweet = t['cleanedText'])\n",
    "    feat2 = getSenti_GPC(lex = gpc, keys = gpcKeys, tweet = t['cleanedText'])\n",
    "    t.update(feat1); t.update( feat2)\n",
    "\n",
    "### export tweets to file\n",
    "keep = Tweets[0].keys()\n",
    "exporter(outfile = \"Tweets_v4\", tweets = RawTweets, keepFields = keep)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Create time series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1 Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countTweetSentiments(data,countVar, byVar, prefixOut):\n",
    "    '''\n",
    "    aggregates categorial data in long-format to counts by category in wide format\n",
    "    params:\n",
    "    - data: pandas data frame\n",
    "    - countVar: categorial data to compute counts by category\n",
    "    - byVar: variable to aggregate on, comes back as index\n",
    "    - prefixOut: counted category labels come back as column labels with this prefix\n",
    "    '''\n",
    "    grpd = pd.DataFrame(data.groupby([byVar,countVar]).size().reset_index())\n",
    "    grpd.columns = [byVar,countVar,'count']\n",
    "    reshaped = grpd.pivot(index=byVar, columns=countVar, values='count')\n",
    "    reshaped.columns = [prefixOut +'_'+ s for s in list(reshaped.columns.values)]\n",
    "    return reshaped\n",
    "\n",
    "def proportions(data,shares,outLabels):\n",
    "    '''\n",
    "    sums up stated columns and computes the share of the sum of each column\n",
    "    params:\n",
    "    - data: pandas data frame\n",
    "    - shares: columns to compute shares\n",
    "    - outLabels: labels of new cols, which hold the shares\n",
    "    '''\n",
    "    Sum = data[shares].sum(axis=1)\n",
    "    for i in range(0,len(shares)):\n",
    "        data[outLabels[i]] = data[shares[i]]/Sum\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2 Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load to data frame\n",
    "TweetsDF = pd.DataFrame(getFromJSONDumpFile('outdata/Tweets_v4.json')) #reload to make this chapter independent\n",
    "\n",
    "#prepare features\n",
    "TweetsDF['monat'] = TweetsDF['created_at'].str[:7]\n",
    "TweetsDF['match'] = TweetsDF['sentiWS_sentiVal'] == TweetsDF['GPC_sentiVal'] #chek match of different senti lexicons\n",
    "TweetsDF['match'] = TweetsDF['match'].astype(str)\n",
    "\n",
    "\n",
    "#aggregate\n",
    "agg1 = countTweetSentiments(data=TweetsDF,countVar='sentiWS_sentiVal', byVar='monat', prefixOut='NbrSentiWS')\n",
    "agg1 = proportions(data=agg1,shares=['NbrSentiWS_positive','NbrSentiWS_negative','NbrSentiWS_neutral'],\n",
    "                   outLabels=['PropSentiWS_positive','PropSentiWS_negative','PropSentiWS_neutral'])\n",
    "agg2 = countTweetSentiments(data=TweetsDF,countVar='GPC_sentiVal', byVar='monat', prefixOut='NbrGPC')\n",
    "agg2 = proportions(data=agg2,shares=['NbrGPC_positive','NbrGPC_negative','NbrGPC_neutral'],\n",
    "                   outLabels=['PropGPC_positive','PropGPC_negative','PropGPC_neutral'])\n",
    "agg3 = TweetsDF[['monat','sentiWS_mean']].groupby('monat').mean().rename(columns={'sentiWS_mean':'sentiWS_meanTot'})\n",
    "agg4 = TweetsDF[['monat','sentiWS_mean']].groupby('monat').sum().rename(columns={'sentiWS_mean':'sentiWS_sumTot'})\n",
    "agg5 = countTweetSentiments(data=TweetsDF,countVar='match', byVar='monat', prefixOut='Match')\n",
    "agg5 = proportions(data=agg5,shares=['Match_True','Match_False'],\n",
    "                   outLabels=['PropMatch_True','PropMatch_False'])\n",
    "\n",
    "TweetsDFAggMonth = pd.concat([agg1,agg3,agg4,agg2,agg5], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.3 Combine with unemployment time series and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get unemployment data\n",
    "unemployment = pd.read_csv('amstat_daten AL/data_arbeitslosigkeit.csv', sep=';').set_index('monat')\n",
    "\n",
    "#match with twitter data\n",
    "TweetsAgg_TimeSeries_v4 = pd.concat([TweetsDFAggMonth,unemployment], axis=1)\n",
    "\n",
    "#save \n",
    "TweetsAgg_TimeSeries_v4.to_pickle('outdata/TweetsAgg_TimeSeries_v4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#XX Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##XX.1 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def completeSeriesDay(incompleteSerie,valueLabel, start='2012-01-01', end='2016-10-31'):\n",
    "    '''\n",
    "    when collecting Twitter data, you may not get data for every day. this function completes time series.\n",
    "    returns a serie object\n",
    "    params:\n",
    "    - imcompleteSeries: Data frame or Serie with datetime index but not every day included, data cols are filled with 0 if missing value\n",
    "    - valueLabel: any string\n",
    "    '''\n",
    "    index = pd.date_range(start, end)\n",
    "    series = pd.Series(index=index)\n",
    "    comb = pd.concat([series,incompleteSerie], axis=1)\n",
    "    comb.drop(comb.columns[0], axis=1, inplace=True) \n",
    "    comb.columns = [valueLabel]\n",
    "    comb.fillna(0, inplace=True)\n",
    "    comb = pd.Series(data=comb[valueLabel], index=index)\n",
    "    return comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##XX.2 Data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "TweetsDF = pd.DataFrame(getFromJSONDumpFile('outdata/Tweets_v4.json')) #reload to make this chapter independent\n",
    "TweetsAgg_TimeSeries_v4 = pd.read_pickle('outdata/TweetsAgg_TimeSeries_v4.pkl') #reload to make the chapter independent\n",
    "\n",
    "\n",
    "#------------\n",
    "#plot number of tweets per day\n",
    "TweetsDF['date'] = pd.to_datetime(TweetsDF['created_at'])\n",
    "TweetsPerDay = pd.Series(TweetsDF.groupby('date').size())\n",
    "combo = completeSeriesDay(incompleteSerie=TweetsPerDay,\n",
    "                          valueLabel='NumberOfTweets')\n",
    "\n",
    "figone= plt.figure(1)\n",
    "plt.yticks(size=7)\n",
    "plt.xticks(rotation=90, size=7)\n",
    "plt.title('Number of Tweets collected per Day', size=20)\n",
    "plt.plot(combo.index, combo,'-')\n",
    "#plt.show()\n",
    "figone.savefig('outfigs/Anzahl_Tweets_Pro_Tag.png')\n",
    "\n",
    "\n",
    "#------------\n",
    "#plot mean and max daily number of tweets per day by month and totals sum of tweets per month\n",
    "mean = combo.resample(rule='M', how='mean')\n",
    "max = combo.resample(rule='M', how=np.max)\n",
    "sum = combo.resample(rule='M', how=np.sum)\n",
    "dat = max.index\n",
    "\n",
    "fig = plt.figure(2)\n",
    "ax = fig.add_subplot(1,1,1)  \n",
    "plt.yticks(size=7)\n",
    "plt.xticks(rotation=90, size=7)\n",
    "plt.title('Number of Tweets collected per Month', size=20)\n",
    "plt.plot(dat,mean,'r-', label='Mean Daily Nbrs of Tweets per Month')\n",
    "plt.plot(dat,sum,'g-', label='Sum of Tweets per Month')\n",
    "plt.plot(dat,max,'g-', label='Max Daily Nbrs of Tweets per Month')\n",
    "plt.grid\n",
    "plt.legend(loc='upper right',prop={'size':10})\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "#ax.xaxis.set_major_locator(mdates.MonthLocator(interval=12))\n",
    "ax.set_yticks(np.arange(0,300,10))\n",
    "ax.grid(which='major')\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m')) \n",
    "#plt.show()\n",
    "fig.savefig('outfigs/Anzahl_Tweets_Pro_Monat_SumMeanMax.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# aggregierten auf Monat:\n",
    "    # pro Monat: tot nbr of senti-vals,  Anteil neg berechnen, jeweils für SentiWS un GPC / mean senti für SentiWS\n",
    "\n",
    "# zusammengesetzte wörter bei sentimentanalyse zerlegen: Stellenabbau --> stellen abbau\n",
    "#random auswahl der tweets gewährleisten? bisher: neuste werden zuerst abgerufen\n",
    "#build new twitter features: number of hashtags, word n-grams, negations, emoticons, elongated words, all caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TRY & ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b  c     a_ant     b_ant\n",
      "0  1  10  x  0.090909  0.909091\n",
      "1  2  20  x  0.090909  0.909091\n",
      "2  3  30  y  0.090909  0.909091\n",
      "3  4  40  y  0.090909  0.909091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = {'a':[1,2,3,4],\n",
    "     'b':[10,20,30,40],\n",
    "    'c':['x','x','y','y']}\n",
    "test = pd.DataFrame(data= d)\n",
    "\n",
    "#data=test\n",
    "#shares=['a','b']\n",
    "#outLabels=['a_ant','b_ant']\n",
    "\n",
    "       \n",
    "test = proportions(data=test,shares=['a','b'],outLabels=['a_ant','b_ant'])\n",
    "\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
